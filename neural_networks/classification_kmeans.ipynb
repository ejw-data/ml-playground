{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some example data\n",
    "np.random.seed(42)\n",
    "num_samples = 1000\n",
    "x_data = np.random.rand(num_samples, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ejwda\\Anaconda3\\envs\\python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "c:\\Users\\ejwda\\Anaconda3\\envs\\python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1382: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Use K-means to cluster the data into K clusters\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "cluster_labels = kmeans.fit_predict(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert cluster labels to one-hot vectors\n",
    "cluster_labels_onehot = np.zeros((num_samples, 3))\n",
    "cluster_labels_onehot[np.arange(num_samples), cluster_labels] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.0361\n",
      "Epoch 2/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0070\n",
      "Epoch 3/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9774\n",
      "Epoch 4/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9449\n",
      "Epoch 5/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9186\n",
      "Epoch 6/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.8951\n",
      "Epoch 7/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.8728\n",
      "Epoch 8/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.8513\n",
      "Epoch 9/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.8301\n",
      "Epoch 10/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.8097\n",
      "Epoch 11/50\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.7900\n",
      "Epoch 12/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7709\n",
      "Epoch 13/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7526\n",
      "Epoch 14/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7351\n",
      "Epoch 15/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7185\n",
      "Epoch 16/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7025\n",
      "Epoch 17/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6874\n",
      "Epoch 18/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6722\n",
      "Epoch 19/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6580\n",
      "Epoch 20/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6442\n",
      "Epoch 21/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6308\n",
      "Epoch 22/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6180\n",
      "Epoch 23/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6058\n",
      "Epoch 24/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5934\n",
      "Epoch 25/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5815\n",
      "Epoch 26/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5700\n",
      "Epoch 27/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5589\n",
      "Epoch 28/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5479\n",
      "Epoch 29/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5377\n",
      "Epoch 30/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.5267\n",
      "Epoch 31/50\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.5164\n",
      "Epoch 32/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5062\n",
      "Epoch 33/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4966\n",
      "Epoch 34/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4869\n",
      "Epoch 35/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4775\n",
      "Epoch 36/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4682\n",
      "Epoch 37/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4591\n",
      "Epoch 38/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4501\n",
      "Epoch 39/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4420\n",
      "Epoch 40/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4330\n",
      "Epoch 41/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4252\n",
      "Epoch 42/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4170\n",
      "Epoch 43/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4090\n",
      "Epoch 44/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4012\n",
      "Epoch 45/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3938\n",
      "Epoch 46/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3867\n",
      "Epoch 47/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3796\n",
      "Epoch 48/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3728\n",
      "Epoch 49/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3658\n",
      "Epoch 50/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3592\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d087d6a1d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a neural network with one hidden layer\n",
    "model = Sequential()\n",
    "model.add(Dense(units=10, activation='relu', input_shape=(2,)))\n",
    "model.add(Dense(units=3, activation='softmax'))\n",
    "\n",
    "# Compile and train the neural network with K-means pseudo-labels\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy')\n",
    "model.fit(x_data, cluster_labels_onehot, epochs=50, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.9169852 , 0.03621232, 0.04680253],\n",
       "       [0.03211839, 0.0997702 , 0.8681113 ],\n",
       "       [0.19196792, 0.6914688 , 0.1165632 ],\n",
       "       ...,\n",
       "       [0.03687795, 0.08184227, 0.88127977],\n",
       "       [0.00867479, 0.18414283, 0.80718243],\n",
       "       [0.50078666, 0.4559316 , 0.04328171]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict(x_data)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_labels_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_class = []\n",
    "for i in predictions.tolist():\n",
    "    max_index = i.index(max(i))\n",
    "    if max_index == 0:\n",
    "        pred_class.append([1,0,0])\n",
    "    elif max_index == 1:\n",
    "        pred_class.append([0,1,0])\n",
    "    else:\n",
    "        pred_class.append([0,0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.895"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(cluster_labels_onehot, pred_class)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
